---
title: "Analysing a Linear Model on the Annual Statistical Report on Homicides in Toronto between 2004 to 2023"
subtitle: "My subtitle if needed"
author: 
  - Sarah Lee
thanks: "Code and data are available at: https://github.com/sarahhhh02/murrumbidgee_paper.git."
date: today
date-format: long
abstract: "This paper analysizes the Anuual Statistical Report on Homicides in Toronto between 2004 to 2023 retrieved fromthe OpenDataToronto portal.  On the whole, the most noticable factor is shootings being the most common homicide method. On top of that, it is shown that weekends are the most prominant days of when Homicide rates are high. The Poisson and Negative Binomial Models are used in this paper, where the Negative Binomial Model is a more accurate choice. The results of the model shows us that the data given does not have strong predictive power indicated by the R^2 and R^2 Adj. are very low. Hence there would need further interpretation to understand the relationship between the variables"
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(opendatatoronto)
library(ggplot2)
library(tidyverse)
library(janitor)
library(dplyr)
library(knitr)
library(rstanarm)

```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false


#### Retrieve Data ####
homicides_clean1 <- read.csv(here::here("data/analysis_data/homicides_clean1.csv"))
homicides_clean2 <- read.csv(here::here("data/analysis_data/homicides_clean2.csv"))
homicides_clean3 <- read.csv(here::here("data/analysis_data/homicides_clean3.csv"))

### Retrieve Model ###
first_model <- readRDS(here::here("models/first_model.rds"))
second_model <- readRDS(here::here("models/second_model.rds"))


```

# Introduction

Homicide rates serve as a crucial indicator of the public's safety as it reveals trends and the distribution of the rates throughout the years. Studying statistics on homicide can motivate the act of public safety and regulations based on the given results and history. This paper explores the distribution of homicide rates between 2004 and 2023 in Toronto along with its predominant methods. The data set on the "Police Annual Statistical Report - Homicides" is retrieved from the Open Data Toronto [@ropendatatoronto] portal website given by Toronto Police reports [@citehomicidedata]. This resource provides a detailed overviews of the city's homicide counts distributed across various expenditure categories. This paper explores the homicide population of the leading methods and days of homicides in Toronto over the years between 2003 and 2023. By identifying the methods of homicides, governments can prioritize research on these methods to decrease the use of them, by enacting law regulations as such. Additionally, police officials by foreseeing the potential days of the week homicides are most likely to occur, they can be extra cautious on those days.

The data is given by a csv. file that includes the categories of location, year, day, month, day of the week, police division, and its unique ID case. The homicide counts are given by the method of homicides, distributed by "shooting", "stabbing", and "other". This paper will mainly focus on the methods of homicides, days of the week, and the years.

With this data set, I plan to use the R programming language [@citeR] with its relevant tools like Tidyverse [@rtidyverse], janitor [@rjanitor], knitr [@rknitr], rstanarm [@rstanarm] and dplyr [@rdplyr]. With the use of this language, I will build linear models for the variables homicide type and the day of the week and consider their results.

# Data {#sec-data}

The major trends and patterns will be analyzed using the tool from ggplot2 [@rggplot2] that will graph the needed information. I will also make use of the tool knitr [@rknitr] to construct tables to give a generalized view on what we are looking for in this paper.The original data set that was retrieved from the Open Data Toronto portal [@ropendatatoronto] which includes all cases of homicides from 2004 to 2023. Taking this data set, I cleaned the data into tree separate csv files. The file homicides_clean1 contains the cleaned file of renaming and only taking the columns needed for this paper. @tbl-homicideperyear demonstrates the cleaned data set and shown below @tbl-homicideperyear in @fig-1 is a linear regression visual of the number of homicides per year through 2004 and 2023.

```{r}
#| message: false
#| echo: false
#| tbl-cap: Total Homicide Cases per Year
#| label: tbl-homicideperyear

# Assuming 'occurrence_year' is the column containing the year information
homicides_by_year <- homicides_clean1 |>
  group_by(year) |>
  summarise(total = n())

knitr::kable(homicides_by_year,'markdown',col.names = c("Year", "Total Homicides"), align = 'c')

```

```{r}
#| label: fig-1
#| fig-cap: Homicides Counts per Year from 2004 to 2023
#| echo: false

base <- 
  homicides_by_year|>
  ggplot(aes(x = year, y = total)) +
  geom_point(alpha = 0.5) +
  labs(
    x = "Year",
    y = "Homicide Count"
  ) +
  theme_classic()
base +
  geom_line() +
  geom_smooth(method = "lm", formula = y ~ x, alpha = 0.3, linetype = "dashed")

```

With the homicides_clean1 data, I combined the year and homicide types to obtain an integer column consisting of the total homicide counts by each homicide type and year, saving this file into homicides_clean2. This new data contains how many counts of homicide there are for each homicide method along with its year. This will give us an overview on which methods of homicides were more predominant in the years. Shown in @fig-2 we are able to see a side to side comparison of the line graph representation of each method of homicides, "shooting", "stabbing", and "other". Just looking at this visualization shows us how much more shooting is the more predominant method.

```{r}
#| label: fig-2
#| fig-cap: Data Plot of Homicides Counts per Year by Homicide Type
#| echo: false

homicides_clean2 |>
  ggplot(mapping = aes(x = year, y = count_type, color = homicide_type)) +
  geom_line() +
  theme_minimal() +
  labs(x = "Year", y = "Homicide Count", color = "Homicide Type") +
  facet_wrap(vars(homicide_type), dir = "v", ncol = 1) +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "none")


```

```{r}
#| label: fig-3
#| fig-cap: Data Plot of Homicide Counts per Year by Day of Week
#| echo: false


homicides_clean3 |>
  ggplot(mapping = aes(x = year, y = count_day, color = day)) +
  geom_point() + 
  theme_minimal() +
  labs(x = "Year", y = "Homicide Count", color = "Day of Week") +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom")

```

Lastly, in another file, homicides_clean3, I combined the year and day of week to again obtain an integer column to see the total homicide counts by year and day of week. Shown below in @fig-3, you can see that the days that are on average predominate in the higher counts of homicide is during the weekends, "Friday", "Saturday", and "Sunday". But we can also see that the highest number of counts of homicide is actually on a "Monday". Hence with this information, we are not able to fully capture the essence of a trend or pattern quite yet.

# Model

In this data set, the dependent variable is the count of homicides which is a non-negative integer, in this case form the cleaned data sets we have count of homicides by type and day of week. On the other hand, the independent variable is the year which is the linear model that is representing a continuous variable. Based on these two information, the appropriate model for this would be either a Poisson Regression or a Negative Binomial Regression.

Both the Poisson and Negative Binomial Regressions are used when assumption of the constant variance in linear regression is violated. Poisson Regression specifically assumes the equality of the mean and variance of the dependent variable are equal. While the Negative Binomial Regression on the other hand allows the variance to exceed the mean.

## Model set-up

The Poisson model is given by

```{=tex}
\begin{align} 
y_i|\lambda_i &\sim \mbox{Poisson}(\lambda_i) \\
log(\lambda_i) & = \beta_0 + \beta_1 \cdot x_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\end{align}
```
and the Negative Binomial model is given by

```{=tex}
\begin{align} 
y_i|\lambda_i, \theta &\sim \mbox{NegativeBinomial}(\mu_i, \theta) \\
log(\mu_i) & = \beta_0 + \beta_1 \cdot x_i\\
\beta_0 &\sim \mbox{Normal}(0, 2.5) \\
\beta_1 &\sim \mbox{Normal}(0, 2.5) \\
\end{align}
```
We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.

### Model justification

The Negative Binomial model is a more accurate representation of the data compared to the Poisson model since Poisson Regression is more prone to errors. If we collect data on the mean and variance from the data plots from earlier. It is shown in @tbl-mean-var-annual, @tbl-mean-var-homicide-type and @tbl-mean-var-day-of-week that all of their variances exceed their means.

```{r}
#| label: tbl-mean-var-annual
#| tbl-cap: Comparison of Mean and Variance of Total Homicides in from 2004 to 2023
#| echo: false
#| warning: false
# Calculate the mean and variance
summary_stats <- homicides_by_year %>%
  summarise(
    Mean = mean(total, na.rm = TRUE),  
    Variance = var(total, na.rm = TRUE) 
  )

kable(summary_stats, caption = "Comparison of Mean and Variance of Total Homicides")
```

```{r}
#| label: tbl-mean-var-homicide-type
#| tbl-cap: Comparison of Mean and Variance of Total Homicides by Type
#| echo: false
#| warning: false
# Calculate the mean and variance
summary_stats <- homicides_clean2 %>%
  summarise(
    Mean = mean(count_type, na.rm = TRUE),  
    Variance = var(count_type, na.rm = TRUE) 
  )

kable(summary_stats, caption = "Comparison of Mean and Variance of Homicides by Type")
```

```{r}
#| label: tbl-mean-var-day-of-week
#| tbl-cap: Comparison of Mean and Variance of Total Homicides by Day of Week
#| echo: false
#| warning: false
# Calculate the mean and variance
summary_stats <- homicides_clean3 %>%
  summarise(
    Mean = mean(count_day, na.rm = TRUE),  
    Variance = var(count_day, na.rm = TRUE) 
  )

kable(summary_stats, caption = "Comparison of Mean and Variance of Homicides by Day of Week")
```

Hence based on these comparisons, it is evident that the model used is correct for this data set.

# Results

Our results for the data on homicides per year based on the homicide type are summarized in @tbl-firstmodelresults. The results on homicides per year based on the day of the week are summarized in @tbl-secondmodelresults.

```{r}
#| echo: false
#| eval: true
#| label: tbl-firstmodelresults
#| tbl-cap: "Explanatory models of Homicides per Year based on Homicide Type"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = first_model
  ),
  statistic = "mad",
  fmt = 2
)
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-secondmodelresults
#| tbl-cap: "Explanatory models of Homicides per Year based on the Day of Week"
#| warning: false

modelsummary::modelsummary(
  list(
    "second model" = second_model
  ),
  statistic = "mad",
  fmt = 2
)
```

Below are the Linear Regression models respectively related with @tbl-firstmodelresults and @tbl-secondmodelresults.

```{r}
#| label: fig-4
#| fig-cap: Linear regression with simulated data on the number of homicides per year, depending on the homicide type 
#| echo: false

base <- 
  homicides_clean2 |>
  ggplot(aes(x = year, y = count_type, color = homicide_type )) +
  labs(
    x = "Year",
    y = "Homicide Count",
    color = "Homicide Type"
  ) +
  theme_classic() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom")

base +
  geom_point() +
  geom_smooth(method = "glm", formula = y ~ x, alpha = 0.3, linetype = "dashed")
```

```{r}
#| label: fig-5
#| fig-cap: Linear regression with simulated data on the number of homicides per year, depending on the day of week
#| echo: false

base <- 
  homicides_clean3 |>
  ggplot(aes(x = year, y = count_day, color = day )) +
  labs(
    x = "Year",
    y = "Homicide Count",
    color = "Day of Week"
  ) +
  theme_classic() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "bottom")

base +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, alpha = 0.3, linetype = "dashed")


```

# Discussion

## First discussion point {#sec-first-point}

## Second discussion point

## Third discussion point

## Weaknesses and next steps

\newpage

\appendix

# Appendix {.unnumbered}

## Posterior predictive check

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(first_model) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(first_model) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```

\newpage

# References
